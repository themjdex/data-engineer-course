{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/24 13:35:53 WARN Utils: Your hostname, themjdex-VirtualBox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "24/07/24 13:35:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/07/24 13:35:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"final_task\").config('spark.master', 'local[*]').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('data/web_server_logs.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ip: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- response_code: integer (nullable = true)\n",
      " |-- response_size: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ip: string (nullable = true)\n",
      " |-- timestamp: date (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- response_code: integer (nullable = true)\n",
      " |-- response_size: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# конвертируем timestamp в тип date\n",
    "df=df.withColumn('timestamp', df['timestamp'].cast(DateType()))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('traffic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 active IP adresses:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+\n",
      "|            ip|request_count|\n",
      "+--------------+-------------+\n",
      "| 207.90.62.173|            2|\n",
      "| 203.50.53.156|            2|\n",
      "|188.56.188.241|            2|\n",
      "|104.225.103.17|            2|\n",
      "| 30.86.184.103|            1|\n",
      "|131.254.187.25|            1|\n",
      "|181.96.201.208|            1|\n",
      "| 199.185.71.89|            1|\n",
      "|  64.18.38.139|            1|\n",
      "|156.25.119.206|            1|\n",
      "+--------------+-------------+\n",
      "\n",
      "Request count by HTTP method:\n",
      "+------+------------+\n",
      "|method|method_count|\n",
      "+------+------------+\n",
      "|  POST|       24862|\n",
      "|DELETE|       25222|\n",
      "|   PUT|       24868|\n",
      "|   GET|       25048|\n",
      "+------+------------+\n",
      "\n",
      "Number of 404 response codes: 25164\n",
      "\n",
      "Total response size by day:\n",
      "+----------+-------------------+\n",
      "|      date|total_response_size|\n",
      "+----------+-------------------+\n",
      "|2024-01-01|            2311466|\n",
      "|2024-01-02|            2391624|\n",
      "|2024-01-03|            2239093|\n",
      "|2024-01-04|            2342243|\n",
      "|2024-01-05|            2440897|\n",
      "|2024-01-06|            2539703|\n",
      "|2024-01-07|            2593665|\n",
      "|2024-01-08|            2583426|\n",
      "|2024-01-09|            2104848|\n",
      "|2024-01-10|            2525125|\n",
      "|2024-01-11|            2446666|\n",
      "|2024-01-12|            2427684|\n",
      "|2024-01-13|            2457414|\n",
      "|2024-01-14|            2387913|\n",
      "|2024-01-15|            2412394|\n",
      "|2024-01-16|            2772024|\n",
      "|2024-01-17|            2388577|\n",
      "|2024-01-18|            2516968|\n",
      "|2024-01-19|            2255118|\n",
      "|2024-01-20|            2323490|\n",
      "+----------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Сгруппируйте данные по IP и посчитайте количество запросов для каждого IP, выводим 10 самых активных IP\n",
    "print('Top 10 active IP adresses:')\n",
    "spark.sql('''\n",
    "          SELECT ip, COUNT(url) AS request_count \n",
    "          FROM traffic \n",
    "          GROUP BY ip \n",
    "          ORDER BY request_count DESC \n",
    "          LIMIT 10\n",
    "          ''').show()\n",
    "\n",
    "# Сгруппируйте данные по HTTP-методу и посчитайте количество запросов для каждого метода\n",
    "print('Request count by HTTP method:')\n",
    "spark.sql('''\n",
    "          SELECT method, COUNT(method) AS method_count \n",
    "          FROM traffic \n",
    "          GROUP BY method \n",
    "          ''').show()\n",
    "\n",
    "# Профильтруйте и посчитайте количество запросов с кодом ответа 404\n",
    "count_404 = spark.sql('''\n",
    "          SELECT COUNT(response_code)\n",
    "          FROM traffic \n",
    "          WHERE response_code = 404 \n",
    "          ''').collect()\n",
    "print(f'Number of 404 response codes: {count_404[0][0]}\\n')\n",
    "\n",
    "# Сгруппируйте данные по дате и просуммируйте размер ответов, сортируйте по дате\n",
    "print('Total response size by day:')\n",
    "spark.sql('''\n",
    "          SELECT timestamp AS date, SUM(response_size) AS total_response_size \n",
    "          FROM traffic \n",
    "          GROUP BY date\n",
    "          ORDER BY date\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "de_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
